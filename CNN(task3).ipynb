{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpKTPszqDym2",
        "outputId": "cc2ff121-1bd5-4d84-f4c5-eb2ddf6c7d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import scipy.special"
      ],
      "metadata": {
        "id": "IS863zsHZdrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/archive/fashion-mnist_train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/archive/fashion-mnist_test.csv')\n",
        "\n",
        "y_train = train_df.iloc[:, 0].values\n",
        "y_test = test_df.iloc[:, 0].values\n",
        "x_train = train_df.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
        "x_test = test_df.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert to torch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long, device=device)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "_iyHMi56Zq-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86bf7c5-7893-435a-b236-3d235cf182d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part (a) Basic CNN testing pooling layers"
      ],
      "metadata": {
        "id": "KzVtSLp7gPJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, pooling_type='max'):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.pooling_type = pooling_type\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
        "\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "        elif pooling_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(2, 2)\n",
        "        elif pooling_type == 'global_avg':\n",
        "            self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid pooling type. Choose from 'max', 'avg', or 'global_avg'\")\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        if self.pooling_type == 'global_avg':\n",
        "            x = x.view(x.size(0), -1)  # Global Average Pooling reduces to [batch_size, 512]\n",
        "        else:\n",
        "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.001):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Train with different pooling methods\n",
        "for pooling in ['max', 'avg', 'global_avg']:\n",
        "    print(f\"Training with {pooling} pooling:\")\n",
        "    model = CNNModel(pooling_type=pooling)\n",
        "    train_model(model, train_loader, test_loader, epochs=10)"
      ],
      "metadata": {
        "id": "kd9oELWnYsbz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437db0de-03f4-404b-a26d-da297ac59902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with max pooling:\n",
            "Epoch 1, Loss: 0.5437493671549917\n",
            "Epoch 2, Loss: 0.3038203815407336\n",
            "Epoch 3, Loss: 0.24829656190709518\n",
            "Epoch 4, Loss: 0.21088486550046182\n",
            "Epoch 5, Loss: 0.17996883856406662\n",
            "Epoch 6, Loss: 0.15714192900981414\n",
            "Epoch 7, Loss: 0.13471318129946525\n",
            "Epoch 8, Loss: 0.11558979905065475\n",
            "Epoch 9, Loss: 0.09618687289091808\n",
            "Epoch 10, Loss: 0.08537116453651267\n",
            "Accuracy: 92.92%\n",
            "Training with avg pooling:\n",
            "Epoch 1, Loss: 0.7426949795375246\n",
            "Epoch 2, Loss: 0.4133508071494001\n",
            "Epoch 3, Loss: 0.33038794565429563\n",
            "Epoch 4, Loss: 0.28641402906478086\n",
            "Epoch 5, Loss: 0.24663612279079872\n",
            "Epoch 6, Loss: 0.22608193463242765\n",
            "Epoch 7, Loss: 0.20681984317518753\n",
            "Epoch 8, Loss: 0.19030366988499153\n",
            "Epoch 9, Loss: 0.17493743482810348\n",
            "Epoch 10, Loss: 0.15861067073399832\n",
            "Accuracy: 92.82%\n",
            "Training with global_avg pooling:\n",
            "Epoch 1, Loss: 1.6014007796356673\n",
            "Epoch 2, Loss: 0.9540242553392707\n",
            "Epoch 3, Loss: 0.7963256694868938\n",
            "Epoch 4, Loss: 0.7161462169084976\n",
            "Epoch 5, Loss: 0.6845568948463082\n",
            "Epoch 6, Loss: 0.6463592629760567\n",
            "Epoch 7, Loss: 0.606490786586489\n",
            "Epoch 8, Loss: 0.5503747278947566\n",
            "Epoch 9, Loss: 0.5241380507376656\n",
            "Epoch 10, Loss: 0.49739121372448103\n",
            "Accuracy: 81.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part (b) CNN testing various combinations of kernels"
      ],
      "metadata": {
        "id": "VexuKJVjgW3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameter combinations\n",
        "kernel_sizes = [3, 5]\n",
        "filter_schemes = [[32, 32, 32, 32, 32], [32, 48, 64, 96, 128], [32, 64, 128, 256, 512]]\n",
        "combinations = list(product(kernel_sizes, filter_schemes))\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for kernel_size, filters in combinations:\n",
        "    padding = kernel_size // 2  # Ensure 'same' padding\n",
        "\n",
        "    class CNNBackbone(nn.Module):\n",
        "        def __init__(self, num_classes=10):\n",
        "            super(CNNBackbone, self).__init__()\n",
        "\n",
        "            self.conv_layers = nn.Sequential(\n",
        "                nn.Conv2d(1, filters[0], kernel_size=kernel_size, stride=1, padding='same'),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(kernel_size=2),\n",
        "\n",
        "                nn.Conv2d(filters[0], filters[1], kernel_size=kernel_size, stride=1, padding='same'),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(kernel_size=2),\n",
        "\n",
        "                nn.Conv2d(filters[1], filters[2], kernel_size=kernel_size, stride=1, padding='same'),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(kernel_size=2),\n",
        "\n",
        "                nn.Conv2d(filters[2], filters[3], kernel_size=kernel_size, stride=1, padding='same'),\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(kernel_size=2),\n",
        "\n",
        "                nn.Conv2d(filters[3], filters[4], kernel_size=kernel_size, stride=1, padding='same'),\n",
        "                nn.ReLU(),\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "            )\n",
        "\n",
        "            self.fc_layers = nn.Sequential(\n",
        "                nn.Linear(filters[4], 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, num_classes)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv_layers(x)\n",
        "            x = torch.flatten(x, start_dim=1)\n",
        "            x = self.fc_layers(x)\n",
        "            return x\n",
        "\n",
        "    # Initialize model\n",
        "    model = CNNBackbone(num_classes=10).to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Evaluate model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Kernel Size: {kernel_size}, Filters: {filters}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = (kernel_size, filters)\n",
        "\n",
        "print(f\"Best Kernel Size: {best_params[0]}, Best Filters: {best_params[1]}, Best Accuracy: {best_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMJy3RYxHasP",
        "outputId": "6375c569-d28a-40ed-eefc-eb6fab8136bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel Size: 3, Filters: [32, 32, 32, 32, 32], Test Accuracy: 87.53%\n",
            "Kernel Size: 3, Filters: [32, 48, 64, 96, 128], Test Accuracy: 90.68%\n",
            "Kernel Size: 3, Filters: [32, 64, 128, 256, 512], Test Accuracy: 92.45%\n",
            "Kernel Size: 5, Filters: [32, 32, 32, 32, 32], Test Accuracy: 90.15%\n",
            "Kernel Size: 5, Filters: [32, 48, 64, 96, 128], Test Accuracy: 91.92%\n",
            "Kernel Size: 5, Filters: [32, 64, 128, 256, 512], Test Accuracy: 92.27%\n",
            "Best Kernel Size: 3, Best Filters: [32, 64, 128, 256, 512], Best Accuracy: 92.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part (c) CNN testing weight initialization methods"
      ],
      "metadata": {
        "id": "d8Bu9e7wgd9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define CNN Model with Avg Pooling and Weight Initialization\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, init_type='random'):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.initialize_weights(init_type)\n",
        "\n",
        "    def initialize_weights(self, init_type):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                if init_type == 'xavier':\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "                elif init_type == 'he':\n",
        "                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                else:  # Random initialization\n",
        "                    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.001):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Train with different weight initialization methods\n",
        "for init_type in ['random', 'xavier', 'he']:\n",
        "    print(f\"Training with {init_type} initialization:\")\n",
        "    model = CNNModel(init_type=init_type)\n",
        "    train_model(model, train_loader, test_loader, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cX1thkWakKo",
        "outputId": "c65e05e3-a90c-4ece-c978-0d7279b00f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training with random initialization:\n",
            "Epoch 1, Loss: 0.6280890377536257\n",
            "Epoch 2, Loss: 0.36096671830489435\n",
            "Epoch 3, Loss: 0.2939871041171713\n",
            "Epoch 4, Loss: 0.253052733647925\n",
            "Epoch 5, Loss: 0.22233603148858175\n",
            "Epoch 6, Loss: 0.19814434830250263\n",
            "Epoch 7, Loss: 0.17855706330396728\n",
            "Epoch 8, Loss: 0.16077871360520182\n",
            "Epoch 9, Loss: 0.14041553848365476\n",
            "Epoch 10, Loss: 0.12432889002554023\n",
            "Accuracy: 93.01%\n",
            "Training with xavier initialization:\n",
            "Epoch 1, Loss: 0.6392001075022764\n",
            "Epoch 2, Loss: 0.3734952395976479\n",
            "Epoch 3, Loss: 0.30425750915366195\n",
            "Epoch 4, Loss: 0.25702474469235587\n",
            "Epoch 5, Loss: 0.2261471711138863\n",
            "Epoch 6, Loss: 0.20126886491868287\n",
            "Epoch 7, Loss: 0.18297598672820242\n",
            "Epoch 8, Loss: 0.16262527127891208\n",
            "Epoch 9, Loss: 0.1469826427574303\n",
            "Epoch 10, Loss: 0.13254878617354485\n",
            "Accuracy: 92.57%\n",
            "Training with he initialization:\n",
            "Epoch 1, Loss: 0.5210387435422015\n",
            "Epoch 2, Loss: 0.31184293266965635\n",
            "Epoch 3, Loss: 0.25017407046420487\n",
            "Epoch 4, Loss: 0.21288156178174242\n",
            "Epoch 5, Loss: 0.18389137810481382\n",
            "Epoch 6, Loss: 0.15943991929602458\n",
            "Epoch 7, Loss: 0.13805923824891575\n",
            "Epoch 8, Loss: 0.11800314607704356\n",
            "Epoch 9, Loss: 0.09654791997487484\n",
            "Epoch 10, Loss: 0.08021769316671892\n",
            "Accuracy: 93.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part (d) CNN extracted features with MLP classification"
      ],
      "metadata": {
        "id": "DuCCb_3SZP-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/archive/fashion-mnist_train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/archive/fashion-mnist_test.csv')\n",
        "\n",
        "y_train = train_df.iloc[:, 0].values\n",
        "y_test = test_df.iloc[:, 0].values\n",
        "x_train = train_df.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
        "x_test = test_df.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert to torch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long, device=device)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define CNN Feature Extractor\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        return x\n",
        "\n",
        "# Train CNN Feature Extractor\n",
        "def train_cnn(model, train_loader, epochs=10, learning_rate=0.001):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Initialize and train CNN Feature Extractor\n",
        "feature_extractor = CNNFeatureExtractor().to(device)\n",
        "train_cnn(feature_extractor, train_loader, epochs=10)\n",
        "\n",
        "# Extract features using CNN\n",
        "def extract_features(model, data_loader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, label in data_loader:\n",
        "            images = images.to(device)\n",
        "            features.append(model(images).cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "    return np.concatenate(features), np.concatenate(labels)\n",
        "\n",
        "train_features, train_labels = extract_features(feature_extractor, train_loader)\n",
        "test_features, test_labels = extract_features(feature_extractor, test_loader)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "train_labels = to_categorical(train_labels, num_classes=10)\n",
        "test_labels = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# Transpose features to match MLP input format\n",
        "train_features = train_features.T\n",
        "test_features = test_features.T\n",
        "train_labels = train_labels.T\n",
        "test_labels = test_labels.T\n",
        "\n",
        "# ======================================================\n",
        "# MLP Classifier\n",
        "# ======================================================\n",
        "\n",
        "# Hyperparameters & Network Architecture\n",
        "hidden_layers = 5\n",
        "input_dim = train_features.shape[0]  # Number of features extracted by CNN\n",
        "hidden_neurons = [256, 256, 256, 256, 256]\n",
        "output_dim = 10\n",
        "\n",
        "# For hidden layers, you can choose any activation among:\n",
        "# \"relu\", \"leaky_relu\", \"tanh\", \"gelu\" : The output layer will always use \"softmax\".\n",
        "activations = ['relu', 'relu', 'relu', 'relu', 'relu', 'softmax']\n",
        "\n",
        "# Build list of layer dimensions\n",
        "layers_dims = [input_dim] + hidden_neurons + [output_dim]\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights = []\n",
        "biases = []\n",
        "for i in range(len(layers_dims) - 1):\n",
        "    if i < len(activations) - 1 and activations[i].lower() in ['relu', 'leaky_relu']:\n",
        "        W = np.random.randn(layers_dims[i+1], layers_dims[i]) * np.sqrt(2.0 / layers_dims[i])\n",
        "    else:\n",
        "        W = np.random.randn(layers_dims[i+1], layers_dims[i]) * np.sqrt(1.0 / layers_dims[i])\n",
        "    b = np.zeros((layers_dims[i+1], 1))\n",
        "    weights.append(W)\n",
        "    biases.append(b)\n",
        "\n",
        "# Activation Functions\n",
        "def activation(x, func=\"relu\", alpha=0.01):\n",
        "    func = func.lower()\n",
        "    if func == \"relu\":\n",
        "        return np.maximum(0, x)\n",
        "    elif func == \"leaky_relu\":\n",
        "        return np.where(x > 0, x, alpha * x)\n",
        "    elif func == \"tanh\":\n",
        "        return np.tanh(x)\n",
        "    elif func == \"gelu\":\n",
        "        return 0.5 * x * (1 + scipy.special.erf(x / np.sqrt(2)))\n",
        "    elif func == \"softmax\":\n",
        "        exps = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation function: \" + func)\n",
        "\n",
        "def activation_derivative(z, func=\"relu\", alpha=0.01):\n",
        "    func = func.lower()\n",
        "    if func == \"relu\":\n",
        "        return (z > 0).astype(float)\n",
        "    elif func == \"leaky_relu\":\n",
        "        return np.where(z > 0, 1.0, alpha)\n",
        "    elif func == \"tanh\":\n",
        "        return 1 - np.tanh(z)**2\n",
        "    elif func == \"gelu\":\n",
        "        return (0.5 * (1 + scipy.special.erf(z / np.sqrt(2))) +\n",
        "                (z * np.exp(-0.5 * z**2) / np.sqrt(2 * np.pi)))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation function for derivative: \" + func)\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X, weights, biases):\n",
        "    activations_list = [X]\n",
        "    pre_activations_list = []\n",
        "    A = X\n",
        "    for i in range(len(weights) - 1):\n",
        "        Z = np.dot(weights[i], A) + biases[i]\n",
        "        pre_activations_list.append(Z)\n",
        "        A = activation(Z, activations[i])\n",
        "        activations_list.append(A)\n",
        "    Z = np.dot(weights[-1], A) + biases[-1]\n",
        "    pre_activations_list.append(Z)\n",
        "    A = activation(Z, \"softmax\")\n",
        "    activations_list.append(A)\n",
        "    return A, activations_list, pre_activations_list\n",
        "\n",
        "# Loss Function (Cross-Entropy)\n",
        "def CE_loss(y, y_pred):\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
        "    loss = -np.sum(y * np.log(y_pred), axis=0)\n",
        "    return np.mean(loss)\n",
        "\n",
        "def loss(X, y, weights, biases):\n",
        "    y_pred, _, _ = forward(X, weights, biases)\n",
        "    return CE_loss(y, y_pred)\n",
        "\n",
        "# Backpropagation\n",
        "def backprop(weights, biases, X, y, learning_rate=0.01):\n",
        "    m = X.shape[1]\n",
        "    y_pred, activations_list, pre_activations_list = forward(X, weights, biases)\n",
        "    dA = y_pred - y\n",
        "\n",
        "    dW_list = []\n",
        "    dB_list = []\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        A_prev = activations_list[i]\n",
        "        dZ = dA\n",
        "        dW = np.dot(dZ, A_prev.T) / m\n",
        "        dB = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        dW_list.insert(0, dW)\n",
        "        dB_list.insert(0, dB)\n",
        "\n",
        "        if i != 0:\n",
        "            dA = np.dot(weights[i].T, dZ)\n",
        "            act_func = activations[i-1]\n",
        "            dZ = dA * activation_derivative(pre_activations_list[i-1], func=act_func)\n",
        "            dA = dZ\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * dW_list[i]\n",
        "        biases[i]  -= learning_rate * dB_list[i]\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# Training Loop\n",
        "def train(X, y, weights, biases, epochs=100, learning_rate=0.01, batch_size=128):\n",
        "    m = X.shape[1]\n",
        "    for epoch in range(epochs):\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[:, permutation]\n",
        "        y_shuffled = y[:, permutation]\n",
        "\n",
        "        for i in range(0, m, batch_size):\n",
        "            end = i + batch_size\n",
        "            X_batch = X_shuffled[:, i:end]\n",
        "            y_batch = y_shuffled[:, i:end]\n",
        "            weights, biases = backprop(weights, biases, X_batch, y_batch, learning_rate)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            current_loss = loss(X, y, weights, biases)\n",
        "            print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# Train MLP Classifier\n",
        "weights, biases = train(train_features, train_labels, weights, biases, epochs=100, learning_rate=0.01, batch_size=128)\n",
        "\n",
        "# Evaluate MLP Classifier\n",
        "def evaluate_mlp(X, y, weights, biases):\n",
        "    y_pred, _, _ = forward(X, weights, biases)\n",
        "    y_pred = np.argmax(y_pred, axis=0)\n",
        "    y_true = np.argmax(y, axis=0)\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    return accuracy\n",
        "\n",
        "test_accuracy = evaluate_mlp(test_features, test_labels, weights, biases)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiLKh5513oqI",
        "outputId": "46879a51-6704-4cb0-c088-a7a0eef8e8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1, Loss: 1.340407866531852\n",
            "Epoch 2, Loss: 1.0304028465231854\n",
            "Epoch 3, Loss: 0.9625205767752011\n",
            "Epoch 4, Loss: 0.9269135058370989\n",
            "Epoch 5, Loss: 0.8972541771845014\n",
            "Epoch 6, Loss: 0.8759593098466076\n",
            "Epoch 7, Loss: 0.8544583037686246\n",
            "Epoch 8, Loss: 0.8364871701896827\n",
            "Epoch 9, Loss: 0.81949006724777\n",
            "Epoch 10, Loss: 0.8075548625990018\n",
            "Epoch 0, Loss: 0.2769\n",
            "Epoch 10, Loss: 0.1508\n",
            "Epoch 20, Loss: 0.1354\n",
            "Epoch 30, Loss: 0.1351\n",
            "Epoch 40, Loss: 0.1389\n",
            "Epoch 50, Loss: 0.1353\n",
            "Epoch 60, Loss: 0.1341\n",
            "Epoch 70, Loss: 0.1304\n",
            "Epoch 80, Loss: 0.1347\n",
            "Epoch 90, Loss: 0.1294\n",
            "Test Accuracy: 93.27%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}